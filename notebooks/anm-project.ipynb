{"cells":[{"metadata":{"_uuid":"3a921f17-ddfd-46af-b2df-89524139c0d5","_cell_guid":"5cf8cdef-6f20-405e-b956-b2c057a3a13e","trusted":true},"cell_type":"code","source":"# 3GB\n# Download training_data\n!wget -O label_training_data.zip https://cloud.tsinghua.edu.cn/f/c1ea3426ce444bc9baae/?dl=1\n\n# Unzip\n!unzip label_training_data.zip -d ./\n\n%cd AIOps挑战赛数据/\n!bash unzip_all.sh\n%cd ..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 885MB\n# Download training_data\n!wget -O training_data.zip https://cloud.tsinghua.edu.cn/f/7eece510dc784e70a083/?dl=1\n\n# Unzip\n!unzip training_data.zip -d ./\n\n# Remove unecessary documents\n%rm -r training_data/__MACOSX/\n%rm training_data.zip\n\n# Check final result\n!ls training_data/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using kaggle dataset\n# Unzip\n!unzip ../input/anm-data/label_training_data.zip -d ./\n\n%cd AIOps挑战赛数据/\n!bash unzip_all.sh\n%cd ..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime, pytz\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pickle\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\ntraining_data_path = \"AIOps挑战赛数据\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Acquire data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport datetime\nimport math\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nclass DataUtils:\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n        self.node_le = LabelEncoder()\n        self.kpi_le = LabelEncoder()\n        self.data_stats = DataStats()\n\n    def reduce_mem_usage(self, df):\n        \"\"\" Function to reduce the DF size \"\"\"\n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n        start_mem = df.memory_usage().sum() / 1024**2\n        for col in df.columns:\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n        end_mem = df.memory_usage().sum() / 1024**2\n        if self.verbose:\n            print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem))\n        return df\n    \n    def load_data(self, training_data_path, dates, nrows=None, dataset='all'):\n        assert dataset in ['all', 'host', 'esb', 'trace']\n        arr_host = []\n        arr_esb = []\n        arr_trace = [] \n        arr_failures = []\n\n        for date in dates:\n            if dataset in ['all', 'esb']:\n                # ESB\n                print(\"{:=^30}\".format('  ESB  '))\n                filename = training_data_path + \"/{}/业务指标/esb.csv\".format(date)\n                print(\"opening {}\".format(filename))\n                esb_df = pd.read_csv(filename, header=0, nrows=nrows)\n\n                esb_df.rename(columns={\n                    'serviceName': 'service_name',\n                    'startTime': 'start_time'},\n                    inplace=True)\n\n                esb_df['start_time'] = pd.to_datetime(\n                    esb_df['start_time'], unit='ms')\n                esb_df['time'] = (esb_df['start_time'] - esb_df['start_time'].min()) / \\\n                    datetime.timedelta(seconds=1)  # should be 0 to 24h in seconds\n                arr_esb.append(esb_df)\n\n            if dataset in ['all', 'host']:\n                # HOST\n                print(\"{:=^30}\".format('  host  '))\n                host_df_lst = []\n                for s_name in [\"db_oracle_11g\", \"dcos_container\", \"dcos_docker\", \"mw_redis\", \"os_linux\"]:\n                    filename = training_data_path + \"/\" + date + \"/平台指标/\" + s_name + \".csv\"\n                    print(\"opening {}\".format(filename))\n                    temp_df = pd.read_csv(filename, header=0)\n                    # rename columns so that is follows convention some_special_name\n                    temp_df.rename(columns={'itemid': 'item_id'}, inplace=True)\n\n                    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'], unit='ms')\n\n                    # changing data type to save memory\n                    temp_df['item_id'] = temp_df['item_id'].astype(int)\n                    temp_df[['cmdb_id', 'bomc_id', 'name']] = temp_df[[\n                        'cmdb_id', 'bomc_id', 'name']].astype('category')\n\n                    host_df_lst.append(temp_df)\n                    del temp_df\n\n                host_df = pd.concat(host_df_lst)\n                del host_df_lst\n\n                arr_host.append(host_df)\n\n            if dataset in ['all', 'trace']:\n                # TRACE\n                print(\"{:=^30}\".format('   trace   '))\n                trace_df_lst = []\n                for c_type in [\"csf\", \"fly_remote\", \"jdbc\", \"local\", \"osb\", \"remote_process\"]:\n                    filename = training_data_path + \"/\" + date + \"/调用链指标/trace_\" + c_type + \".csv\"\n                    print(\"opening {}\".format(filename))\n                    # chunks = pd.read_csv(filename, header=0, chunksize=100000)\n                    # temp_df = pd.concat(chunks)\n                    temp_df = pd.read_csv(filename, header=0, nrows=nrows)\n                    temp_df.rename(\n                        columns={\n                            'callType': 'call_type',\n                            'startTime': 'start_time',\n                            'elapsedTime': 'elapsed_time',\n                            'traceId': 'trace_id',\n                            'serviceName': 'service_name',\n                            'dsName': 'ds_name'},\n                        inplace=True)  # rename columns so that is follows convention some_special_name except for Id\n                    temp_df['start_time'] = pd.to_datetime(temp_df['start_time'], unit='ms')\n                    trace_df_lst.append(temp_df)\n                    del temp_df\n\n                trace_df = pd.concat(trace_df_lst)\n                arr_trace.append(trace_df)\n\n            # FAILURES\n            print(\"{:=^30}\".format('   failures   '))\n            filename = \"故障整理（预赛）.csv\"\n            print(f\"opening {filename}\")\n            # read csv\n            failures_df = pd.read_csv(filename, index_col='index')\n\n            # interpret as datetime objects\n            failures_df['start_time'] = pd.to_datetime(\n                failures_df['start_time'], format='%Y/%m/%d %H:%M', infer_datetime_format=True).dt.tz_localize('Asia/Shanghai').dt.tz_convert(None)\n            failures_df['log_time'] = pd.to_datetime(\n                failures_df['log_time'], format='%Y/%m/%d %H:%M', infer_datetime_format=True).dt.tz_localize('Asia/Shanghai').dt.tz_convert(None)\n\n            # load failures for the day date\n            datetime_date = datetime.datetime.strptime(date, \"%Y_%m_%d\").date()\n            failures_df = failures_df[failures_df['start_time'].dt.date == datetime_date]\n            arr_failures.append(failures_df)\n        \n\n        if dataset == 'all':\n            return pd.concat(arr_esb), pd.concat(arr_host), pd.concat(arr_trace), pd.concat(arr_failures)\n        else: \n            dataset_arr_dict = {'esb': arr_esb, 'host': arr_host, 'trace': arr_trace}\n            return pd.concat(dataset_arr_dict[dataset]), pd.concat(arr_failures)\n    \n    def add_timeseries_features(self, df, w_period, w_time_unit, w_length):\n        \"\"\"\n        Convert Timeseries data to supervised dataset by shifting data.\n        Params: \n            - df: host_df\n            - w_period: int, window period ie. how frequent you want to sample data\n            - w_time_unit: str, time unit for w_perdiod\n            - w_length: int, window length ie. how far in the past we want to see\n        \n        Returns:\n            - train_df: transformed df with new features and target_kpi/node/value\n            - supervised_dataset: transformed with new features only (train_df.shape[0] == supervised_dataset.shape[0]*#of_kpis)\n        \n        Example:\n            (df, 1, 'min', 5) you want your model to predict the next value based on the previous 5minutes with data sampling every minute (thus based on 4 values)\n        \n        Note: this code is designed for the global xgboost model\n        \"\"\"\n\n        # Check parameters\n        supported_time_unit = ['h', 'min', 's', 'ms']\n        assert w_time_unit in supported_time_unit, \"Only {} are supported for w_time_unit\".format(supported_time_unit)\n\n        # Gather data\n        temp_df = df.loc[:, ['cmdb_id', 'name', 'timestamp', 'value']]\n        temp_df['timestamp'] = temp_df['timestamp'].dt.round('30s')  # reduce precision of timestamp\n\n\n        # Create features: all kpis ('name') become features\n        temp_df = pd.pivot_table(temp_df, index=['timestamp', 'cmdb_id'], columns='name', values='value', dropna=True)\n        temp_df.reset_index(['cmdb_id'], inplace=True)\n\n        # Fill missing data using forward fill\n        temp_df.fillna(method='ffill', inplace=True)\n\n        # Drop rows with Nan values\n        temp_df.dropna(axis=0, how='any',inplace=True)\n\n        # Reduce mem usage\n        temp_df = self.reduce_mem_usage(temp_df)\n        \n        # Shift data and create even more features (e.g. 'CPU_used (t-3min)')\n        #  while creating supervised_dataset\n        supervised_dataset = temp_df.reset_index()\n        for i in range(1, w_length + w_period, w_period):\n            s = temp_df.shift(periods=i, freq=w_time_unit)\n            s.columns = [\"{}(t-{}{:s})\".format(_n, i, w_time_unit) for _n in s.columns]\n            s.reset_index(inplace=True)\n\n            supervised_dataset = pd.merge(supervised_dataset, s, left_on=['cmdb_id', 'timestamp'], right_on=['cmdb_id(t-{}{:s})'.format(i, w_time_unit), 'timestamp'])\n            supervised_dataset.drop('cmdb_id(t-{}{:s})'.format(i, w_time_unit), axis=1, inplace=True)\n\n        # Drop Nan and adding time features\n        supervised_dataset.dropna(how='any', inplace=True)          \n        supervised_dataset['hour'] = supervised_dataset['timestamp'].dt.hour\n        supervised_dataset.drop('timestamp', axis=1, inplace=True)\n\n        # Check missing data\n        percent_missing = self.data_stats.missing_statistics(temp_df)\n        # print(percent_missing)\n\n        return supervised_dataset\n    \n    def to_multiindex(self, df, unique_kpi, w_period, w_time_unit, w_length):\n        # pandas multi index\n        time_idx = [\"t\"] + [f\"t-{i}{w_time_unit}\" for i in range(1, w_length + w_period, w_period)]\n        idx = pd.MultiIndex.from_product([time_idx, unique_kpi], names=['time', 'kpi'])\n        \n        # values\n        vals = df.loc[:, ~df.columns.isin(['hour', 'cmdb_id'])].values\n\n        # transform to multi index\n        multiindex_supervised_dataset = pd.DataFrame(vals, columns=idx)\n\n        return multiindex_supervised_dataset\n    \n    def to_tensor(self, multi_df, w_length):\n        # shape = (samples, timesteps, features)\n        shape = (multi_df['t'].shape[0], w_length+1, multi_df['t'].shape[1])\n        tensor = multi_df.values.reshape(shape)\n        return tensor\n    \n    def transform_to_lstm_data(self, df, unique_kpi, w_period, w_time_unit, w_length, scaler=None):\n        # supervised dataset\n        supervised_dataset = self.add_timeseries_features(df, w_period, w_time_unit, w_length)\n        \n        # get values\n        supervised_dataset = supervised_dataset.loc[:, ~supervised_dataset.columns.isin(['hour', 'cmdb_id'])].values\n\n        # scale data and transform to tensor\n        assert supervised_dataset.shape[0] > 0, \"Supervised dataset is empty !\"\n        if scaler:\n            supervised_dataset = scaler.fit_transform(supervised_dataset)\n        \n        # shape = (samples, timesteps, features)\n        n_samples = supervised_dataset.shape[0]\n        n_timesteps = w_length+1\n        n_features = unique_kpi.shape[0]\n        shape = (n_samples, n_timesteps, n_features)\n        tensor = supervised_dataset.reshape(shape)\n        return tensor\n            \n    def create_ts_files(self, df, history_length, step_size, lag_unit, target_step, data_folder, num_rows_per_file):\n        \"\"\" like add_timeseries_features but creates files along the way \"\"\"\n        # Check parameters\n        supported_time_unit = ['h', 'min', 's', 'ms']\n        assert lag_unit in supported_time_unit, f\"Only {supported_time_unit} are supported for w_time_unit\"\n\n        # Gather data\n        temp_df = df.loc[:, ['cmdb_id', 'name', 'timestamp', 'value']]\n        temp_df['timestamp'] = temp_df['timestamp'].dt.round('30s')  # reduce precision of timestamp\n\n        # Create features: all kpis ('name') become features\n        temp_df = pd.pivot_table(temp_df, index=[\n                                 'timestamp', 'cmdb_id'], columns='name', values='value', dropna=True)\n        temp_df.reset_index(['cmdb_id'], inplace=True)\n\n        # Fill missing data using forward fill\n        temp_df.fillna(method='ffill', inplace=True)\n\n        # Drop rows with Nan values\n        temp_df.dropna(axis=0, how='any', inplace=True)\n\n        # Reduce mem usage\n        temp_df = self.reduce_mem_usage(temp_df)\n\n        # CREATE FILES\n        num_rows = len(temp_df)\n        num_files = math.ceil(num_rows/num_rows_per_file)\n\n        print(f'Creating {num_files} files.')\n        for i in range(num_files):\n            filename = f'{data_folder}/ts_file{i}.pkl'\n            \n            if i % 10 == 0:\n                print(f'{filename}')\n            \n            left_idx = i * num_rows_per_file\n            right_idx = min(left_idx + num_rows_per_file, num_rows)\n\n\n            # Shift data and create even more features (e.g. 'CPU_used (t-3min)')\n            #  while creating supervised_dataset\n            supervised_dataset = temp_df.iloc[left_idx:right_idx].reset_index()\n            for i in range(history_length, 0, -step_size):\n                s = temp_df.shift(periods=i, freq=lag_unit)\n                s.columns = [\"{}(t-{}{:s})\".format(_n, i, lag_unit)\n                            for _n in s.columns]\n                s.reset_index(inplace=True)\n\n                supervised_dataset = pd.merge(supervised_dataset, s, left_on=['cmdb_id', 'timestamp'], right_on=[\n                                            'cmdb_id(t-{}{:s})'.format(i, lag_unit), 'timestamp'])\n                supervised_dataset.drop(\n                    'cmdb_id(t-{}{:s})'.format(i, lag_unit), axis=1, inplace=True)\n\n            # Drop Nan and adding time features\n            supervised_dataset.dropna(how='any', inplace=True)\n            supervised_dataset['hour'] = supervised_dataset['timestamp'].dt.hour\n            supervised_dataset.drop('timestamp', axis=1, inplace=True)\n\n            supervised_dataset.to_pickle(filename)\n\n        num_ts = temp_df.shape[1]\n        num_timesteps = history_length\n        return num_timesteps, num_ts\n    \n    def create_training_data(self, supervised_dataset, save_dir=\"./\"):\n        \"\"\"\n        Returns:\n            - filenames: list of filename where training data is saved\n                \n        \"\"\"\n        unique_kpi = supervised_dataset.columns.str.extract(r'([a-zA-Z_\\s]+)')[0].unique()\n        unique_kpi = unique_kpi[~np.isin(unique_kpi, ['hour', 'cmdb_id', 'target_node', 'target_value', 'target_kpi'])]\n        \n        # Create train_df\n        print(\">The derived supervised_dataset has {} rows, now creating train_df with {}x more rows ({}).\".format(\n            supervised_dataset.shape[0], \n            unique_kpi.shape[0],\n            unique_kpi.shape[0] * supervised_dataset.shape[0])\n            )\n        supervised_dataset.rename({ 'cmdb_id':  'target_node'}, axis=1, inplace=True)\n\n        names = supervised_dataset.columns.tolist() + ['target_value', 'target_kpi']\n        train_df = pd.DataFrame(columns=names)\n        filenames = []\n        i = 1\n        for _kpi in unique_kpi:\n            temp_df = supervised_dataset.rename({   _kpi: 'target_value'}, axis=1)\n            temp_df['target_kpi'] = _kpi\n            train_df = train_df.append(temp_df, ignore_index=True)\n\n            memory = train_df.memory_usage().sum() / 1024**2\n            if memory > 1000: # save every 1Go\n                filename = save_dir + \"training_data{}\".format(i)\n                print(\">Saving {:.1f} MB of data into {:s}\".format(memory, filename))\n                train_df.to_parquet(filename, index=None)\n                filenames.append(filename)\n                i += 1\n                del train_df\n                train_df = pd.DataFrame(columns=names)\n                gc.collect()\n        train_df.rename({'cmdb_id':'target_node'}, axis=1, inplace=True)\n\n        return filenames\n    \n    def scale_data(self, supervised_dataset):\n        unique_kpi = supervised_dataset.columns.str.extract(r'([a-zA-Z_\\s]+)')[0].unique()\n        unique_kpi = unique_kpi[~np.isin(unique_kpi, ['hour', 'cmdb_id', 'target_node', 'target_value', 'target_kpi'])]\n        \n        self.scaler = MinMaxScaler(feature_range=(1, len(unique_kpi)))\n        return self.scaler.fit_transform(supervised_dataset)        \n    \n    def create_testing_data(self, supervised_dataset, node, kpi):\n        test_df = supervised_dataset[supervised_dataset['cmdb_id'] == node]\n        test_df.rename({ 'cmdb_id': 'target_node', kpi: 'target_value'}, axis=1, inplace=True)\n        test_df['target_kpi'] = kpi\n        return test_df\n            \n#################################################################################\n#                           DATA STATISTICS CLASS\n#################################################################################\nclass DataStats:\n    def __init__(self):\n        pass\n\n    # source: https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling\n    def missing_statistics(self, df):\n        statitics = pd.DataFrame(df.isnull().sum()).reset_index()\n        statitics.columns = ['COLUMN NAME', \"MISSING VALUES\"]\n        statitics['TOTAL ROWS'] = df.shape[0]\n        statitics['% MISSING'] = round(\n            (statitics['MISSING VALUES']/statitics['TOTAL ROWS'])*100, 2)\n        return statitics\n\n    def infinite_statistics(self, df):\n        statitics = pd.DataFrame(df[df.abs() >= np.inf].sum()).reset_index()\n        statitics.columns = ['COLUMN NAME', \"INFINITE VALUES\"]\n        statitics['TOTAL ROWS'] = df.shape[0]\n        statitics['% INFINITE'] = round(\n            (statitics['INFINITE VALUES']/statitics['TOTAL ROWS'])*100, 2)\n        return statitics\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_utils = DataUtils(verbose=True)\nesb_df, host_df, trace_df, failures_df = data_utils.load_data(training_data_path, [\"2020_05_23\"], 1000, 'all') \n\nfailure_free_host_df = host_df[host_df['timestamp'] > failures_df['start_time'].max()]\nfailure_free_host_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_utils = DataUtils(verbose=True)\nhost, failures = data_utils.load_data(training_data_path, [\"2020_05_23\"], 1000, 'host')\n\nsizes = []\nfor _ in range(100):\n\n    rdm_tmsp = np.random.choice(host['timestamp'].values, size=1, replace=True)[0]\n\n    small_host = host[(host['timestamp']-rdm_tmsp).abs() < datetime.timedelta(minutes=6)]\n    supervised_dataset = data_utils.add_timeseries_features(small_host, 1, 'min', 5)\n    sizes.append(supervised_dataset.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(np.where(np.array(sizes)==0)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test with one kpi"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST WITH ONE KPI (container_cpu_used)\nCURR_KPI = 'container_cpu_used'\n\n# TRANSFORM DATA\nprint(\"Gathering data\")\nd = failure_free_host_df[failure_free_host_df['name'] == CURR_KPI][['cmdb_id', 'timestamp', 'value']]\nd['timestamp'] = d['timestamp'].dt.round('60s') # reduce precision of timestamp\nd['hour'] = d['timestamp'].dt.hour\n\n# print(\"Here is a plot of every kpi ({nb_of_kpis}) for the node 'db_008'\".format(nb_of_kpis=d['name'].unique().shape[0]))\n# g = sns.FacetGrid(data=d, row='name', sharex=False, sharey=False, height=2, aspect=4)\n# g.map_dataframe(sns.lineplot, x='timestamp', y='value')\n\npivot_d = pd.pivot_table(d, index='timestamp', columns='cmdb_id', values='value', dropna=True) # create columns out of every kpi 'name'\npercent_missing = pivot_d.isnull().sum() * 100 / len(pivot_d)\nprint(\"Dropping {} columns because percent_missing is over 50%\".format(pivot_d.loc[:, percent_missing >= 50].shape[1]))\npivot_d = pivot_d.loc[:, percent_missing < 50]  # drop columns that have too much missing values\n\n# # alternatively\n# print(\"Filling NaN using forward fill (ffill) method\")\n# pivot_d = pivot_d.fillna(method='ffill')\n# pivot_d = pivot_d.dropna(axis=0, how='any')\n\narr_of_df_with_past_values = []\nfor i in range(5, 0, -1):\n  temp_df = pivot_d.shift(periods=i, freq='min')\n  temp_df.columns = [\"{}(t-{}min)\".format(_n, i) for _n in temp_df.columns]\n  temp_df.dropna(axis=0, how='all', inplace=True)   # drop rows that are empty   \n  arr_of_df_with_past_values.append(temp_df)\n\ntrain_df_arr = []\nfor _node in pivot_d.columns:\n  # current values of other kpis (not the one we are going to predict)\n  temp_df = pd.concat(arr_of_df_with_past_values + [pivot_d.loc[:, pivot_d.columns!=_node]])\n  temp_df['target_node'] = _node          # fills target_kpi colum\n  temp_df['target_value'] = pivot_d[_node]\n  train_df_arr.append(temp_df)\n\n# create train_df\ntrain_df = pd.concat(train_df_arr)\ntrain_df = train_df.fillna(method='ffill')    # fill nan with previous value (ffill = foward fill)\ntrain_df = train_df.dropna(axis=0, how='any') # drop nan\ntrain_df['target_node'] = train_df['target_node'].astype('category')    # categorize the target columns\ntrain_df = pd.get_dummies(train_df, columns=['target_node'], prefix=['tgt_node'])\ntrain_df = train_df.reset_index().drop(columns=['timestamp'])\n\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters tuning using genetic algorithm\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nHYPERPARAMETERS TUNING USING GENETIC ALGORITHM\n\"\"\"\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom scipy.optimize import differential_evolution\n\nclass Tuner:\n    def __init__(self, params_lim, model_init, fixed_params, eval_metrics, kfold_splits, dtypes, seed=None):\n        self.params_lim = params_lim\n        self.params = list(params_lim.keys())\n        self.model_init = model_init\n        self.fixed_params = fixed_params\n        self.kfold_splits = kfold_splits\n        self.dtypes=dtypes\n        self.seed = seed\n\n    def _function(self, hyperparams, X, y):\n        # Assign hyper-parameters\n        model_params = {}\n        for _name, _value in zip(self.params, hyperparams):\n            if self.dtypes[_name] == 'int':\n                model_params[_name] = int(_value)\n            else:\n                model_params[_name] = _value\n\n        ## Using kfold cross validation\n        if self.seed:\n            kf = KFold(n_splits=self.kfold_splits, shuffle=True, random_state=self.seed)\n        else :\n            kf = KFold(n_splits=self.kfold_splits, shuffle=False)\n\n        y_pred_total = []\n        y_test_total = []\n        # kf-fold cross-validation loop\n        for train_index, test_index in kf.split(X):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n            # Fit xgboost with (X_train, y_train), and predict X_test\n            model = self.model_init(**self.fixed_params, **model_params)\n            y_pred = model.fit(X_train, y_train).predict(X_test)\n            # Append y_pred and y_test values of this k-fold step to list with total values\n            y_pred_total.append(y_pred)\n            y_test_total.append(y_test)\n        # Flatten lists with test and predicted values\n        y_pred_total = [item for sublist in y_pred_total for item in sublist]\n        y_test_total = [item for sublist in y_test_total for item in sublist]\n\n        # Calculate error metric of test and predicted values: rmse\n        rmse = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n\n        # log message\n        print(\">Intermediate results: rmse: {}, {}\".format(rmse, model_params))\n        return rmse\n    \n    def tune_hyperparameters(self, X, y, popsize=10, mutation=0.5, recombination=0.7, tol=0.1, workers=1, maxiter=100, init=None):\n        # params boundaries\n        boundaries = [value for value in self.params_lim.values()]\n\n        # extra variables\n        extra_variables = (X, y)\n\n        ## set up Differential Evolution solver\n        if init: \n            solver = differential_evolution(\n                func=self._function, \n                bounds=boundaries, \n                args=extra_variables, \n                strategy='best1bin',\n                popsize=popsize, \n                mutation=mutation, \n                recombination=recombination, \n                tol=tol, \n                seed=self.seed, \n                workers=workers, \n                maxiter=maxiter,\n                init=init\n                )\n        else:\n            solver = differential_evolution(\n                func=self._function,\n                bounds=boundaries,\n                args=extra_variables,\n                strategy='best1bin',\n                popsize=popsize,\n                mutation=mutation,\n                recombination=recombination,\n                tol=tol,\n                seed=self.seed,\n                workers=workers,\n                maxiter=maxiter\n            )\n\n        ## calculate best hyperparameters and resulting rmse\n        best_hyperparams = solver.x\n        best_rmse = solver.fun\n\n        ## print final results\n        print(\"Converged hyperparameters: {}\".format(best_hyperparams))\n        print(\"Minimum rmse: {:.6f}\".format(best_rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPERPARAMETER TUNING WITH GENETIC ALGORITHM\n##  training data\nX = train_df.loc[:, train_df.columns != 'target_value']\ny = train_df['target_value']\n\n# for regularization params (reg_alpha, reg_lambda)\nfixed_params ={\n    'learning_rate': 0.1,\n    'gamma': 43,\n    'max_depth': 4,\n    'colsample_bytree': 0.4,\n    'min_child_weight': 98,\n    'base_score': 0.5,\n    'booster': 'gbtree',\n    'importance_type': 'gain',\n    'n_estimators': 50,\n    'nthread': 1,\n    'objective': 'reg:squarederror',\n    'seed': 2020,\n    'subsample': 1,\n    'verbosity': 1\n    }\n\nparams_lim = {\n    'reg_alpha' : (0, 10),\n    'reg_lambda' : (0, 10)\n}\n\ndtypes = {\n    'reg_alpha': 'float',\n    'reg_lambda': 'float'\n}\n\n# for learning_rate, gamma, max_depth, colsample_bytree and min_child_weight\nfixed_params ={\n    'base_score': 0.5,\n    'booster': 'gbtree',\n    'importance_type': 'gain',\n    'n_estimators': 50,\n    'nthread': 1,\n    'objective': 'reg:squarederror',\n    'reg_alpha': 9.2,\n    'reg_lambda': 8.9,\n    'seed': 2020,\n    'subsample': 1,\n    'verbosity': 1\n    }\n\nparams_lim = {\n    'learning_rate' : (0.00001, 1),\n    'gamma' : (0, 100),\n    'max_depth' : (3, 20),\n    #'reg_alpha' : (0, 10),\n    #'reg_lambda' : (0, 10),\n    'colsample_bytree' : (0.1, 1),\n    'min_child_weight' : (0, 100),\n}\n\ndtypes = {\n    'learning_rate' : 'float',\n    'gamma' : 'float',\n    'max_depth' : 'int',\n    'colsample_bytree' : 'float',\n    'min_child_weight' : 'float',\n}\n\n\n\ntuner = Tuner(params_lim=params_lim, model_init=xgb.XGBRegressor, fixed_params=fixed_params, eval_metrics='rmse', kfold_splits=8, dtypes=dtypes)\ntuner.tune_hyperparameters(X, y, popsize=10, mutation=0.5, recombination=0.7, tol=0.1, workers=4, maxiter=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params ={\n    'base_score': 0.5,\n    'booster': 'gbtree',\n    'importance_type': 'gain',\n    'n_estimators': 100,\n    'nthread': 4,\n    'objective': 'reg:squarederror',\n    'reg_alpha': 9.2,\n    'reg_lambda': 8.9,\n    'seed': 2020,\n    'subsample': 1,\n    'verbosity': 1,\n    'learning_rate':0.1, \n    'gamma': 43, \n    'max_depth':10,\n    'colsample_bytree':0.4,\n    'min_child_weight':98\n    }\n##  training data\nX = train_df.loc[:, train_df.columns != 'target_value']\ny = train_df['target_value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n\nmodel = xgb.XGBRegressor(**params)\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'])\nres = model.evals_result()\nn = len(res['validation_0']['rmse'])\n\nsns.lineplot(x=np.arange(n), y=res['validation_1']['rmse'])\n\nplt.plot(model.predict(X_test), 'b-', y_test.reset_index()['target_value'], 'r--', alpha=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MultiRegressor "},{"metadata":{"trusted":true},"cell_type":"code","source":"# FIT XGBOOST MODEL (multiregression)\nfrom sklearn.multioutput import MultiOutputRegressor\n\nprint(\">Loading data\")\ndata_utils = DataUtils(verbose=True)\nday_train = \"2020_05_24\"\nday_test = \"2020_05_25\"\n\nday_train_dt = datetime.datetime.strptime(day_train, \"%Y_%m_%d\")\nday_test_dt = datetime.datetime.strptime(day_test, \"%Y_%m_%d\")\n\nhost, failures = data_utils.load_data(training_data_path, [day_train, day_test], None, 'host')\n\ntrain_host = host[host['timestamp'].dt.date == day_train_dt.date()]\ntest_host = host[host['timestamp'].dt.date == day_test_dt.date()]\n\n##  training data\nprint(\">Preparing data\")\ntrain_ddf = data_utils.add_timeseries_features(train_host, 1, 'min', 5)\ntrain_ddf['cmdb_id'] = data_utils.node_le.fit_transform(train_ddf['cmdb_id'])\n\nunique_kpi = host['name'].unique()\n\nX = train_ddf.loc[:, ~train_ddf.columns.isin(unique_kpi)]\ny = np.log1p(train_ddf.loc[:, unique_kpi])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False, random_state=2020)\n\n## fit model\nprint(\">Initializing model\")\nparams = {\n    'learning_rate': 0.910, \n    'gamma': 50.107, \n    'max_depth': 18, \n    'colsample_bytree': 0.814, \n    'min_child_weight': 0.967,\n    }\nmodel = xgb.XGBRegressor(\n    objective='reg:squarederror', \n    n_estimators=10,\n    **params)\n\n\nprint(\">Fitting model\")\nfit_params = {\n    'eval_set':[(X_train, y_train), (X_test, y_test)], \n    'eval_metric':'rmse', \n    'verbose': 3\n}\n\nmultioutputregressor = MultiOutputRegressor(model, n_jobs=2).fit(X_train, y_train)\n\n## save model\nprint(\">Saving\")\nfilename = \"multioutput_xgboost.pkl\"\npickle.dump(multioutputregressor, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multioutputregressor = pickle.load(open('../input/multioutput-xgboost/multioutput_xgboost.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 400\ny_predicted = multioutputregressor.predict(X_test.iloc[:N])\n\nrmsle = np.sqrt((y_predicted-y_test.iloc[:N])**2)\nrmsle['cmdb_id'] = X_test.iloc[:N]['cmdb_id']\n\nconclusion = rmsle.groupby('cmdb_id').mean()\ndetected_anomalies_rmse = conclusion[conclusion > 1].dropna(axis=0, how='all').dropna(axis=1, how='all')\nnodes = data_utils.node_le.inverse_transform(detected_anomalies_rmse.index)\ndetected_anomalies_rmse.index = data_utils.node_le.inverse_transform(detected_anomalies_rmse.index)\n\nsubmission = []\nfor n in nodes:\n    kpi = detected_anomalies_rmse.loc[n, :].dropna().axes[0]\n    for k in kpi:\n        submission.append({'timestamp': host['timestamp'].max(), 'content': (n, k)})\n\nsubmission_df = pd.DataFrame(submission)\nsubmission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CURR_NODE = 42\nN = None\ny_predicted = multioutputregressor.predict(X_test.iloc[:N])\n\npredicted = pd.DataFrame(y_predicted, columns=unique_kpi)\npredicted['cmdb_id'] = X_test.iloc[:N, X_test.columns=='cmdb_id'].values\ntrue = y_test.iloc[:N]\ntrue['cmdb_id'] = X_test.iloc[:N]['cmdb_id'].values\n\nn_figs = 5\nplot_predicted = predicted[predicted['cmdb_id']==CURR_NODE].iloc[:, -n_figs-1:-1]\nplot_true = true[true['cmdb_id']==CURR_NODE].iloc[:, -n_figs-1:-1]\n\nfig, ax = plt.subplots(n_figs, 2, figsize=(20, 20))\n# plot_input.plot(ax=ax[:, 0], subplots=True, sharey=False, xlabel=\"input\")\nplot_true.plot(ax=ax[:, 0], subplots=True, sharey=False, sharex=True, xlabel=\"true\")\nplot_predicted.plot(ax=ax[:, 1], subplots=True, sharey=False, xlabel=\"predicted\")\nfig.suptitle(\"KPIs of host 'os_020'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig.savefig('multioutput_xgboost_os020.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lstm\n- one model for each host\n- multivariate time series forecasting\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOAD DATA\nprint(\">Loading data\")\ndata_utils = DataUtils(verbose=True)\nday_train = \"2020_05_23\" # china time\nday_test = \"2020_05_24\" # china time\ntz = pytz.timezone('Asia/Shanghai')\n\nday_train_dt = tz.localize(datetime.datetime.strptime(day_train, \"%Y_%m_%d\")).astimezone(pytz.utc)\nday_test_dt = tz.localize(datetime.datetime.strptime(day_test, \"%Y_%m_%d\")).astimezone(pytz.utc)\n\nhost, failures = data_utils.load_data(training_data_path, [day_train, day_test], None, 'host')\nnodes = host['cmdb_id'].unique()\n\nhost['cmdb_id'] = data_utils.node_le.fit_transform(host['cmdb_id'])\n\ntrain_host = host[host['timestamp'].dt.date == day_train_dt.date()]\ntest_host = host[host['timestamp'].dt.date == day_test_dt.date()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_failures = failures[failures['start_time'].dt.date == day_test_dt.date()]\ntest_failures = test_failures[test_failures['name'] == 'os_020']\ntmsp = test_failures['start_time'].iloc[0]\ntest_host = test_host[(test_host['timestamp'] - tmsp).abs() < datetime.timedelta(hours=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(anomalous_test_host[anomalous_test_host['cmdb_id'] == 42]['name'].unique() )\nprint(\"=\"*50)\nprint(test_host[test_host['cmdb_id'] == 42]['name'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_host[(test_host['cmdb_id'] == 42) & (test_host['name'] == 'Memory_total')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRANSFORM DATA\nWIN_PERIOD = 1\nWIN_LENGTH = 5\nTIME_UNIT = \"min\"\n\n# RESHAPE DATA FOR LSTM\nprint(\">Reshaping data for LSTM\")\nfor node_lbl, node in zip(sorted(nodes), data_utils.node_le.transform(sorted(nodes))):\n    print(f\">>Examining node {node_lbl} (n°{node})\")\n    temp = train_host[train_host['cmdb_id'] == node]\n    kpi = temp['name'].unique()\n    \n    assert temp.shape[0] > 0, \"temp is empty\"\n    scaler = StandardScaler()\n    tensor = data_utils.transform_to_lstm_data(temp, kpi, WIN_PERIOD, TIME_UNIT, WIN_LENGTH, scaler)\n    print(f\"(tensor shape: {tensor.shape})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nCURR_NODE = 42 # os_020\ntrain = train_host[train_host['cmdb_id'] == CURR_NODE]\ntest = test_host[test_host['cmdb_id'] == CURR_NODE]\nkpi = train['name'].unique()\ntest_kpi = test['name'].unique()\n\nassert train.shape[0] > 0, \"train is empty\"\nassert test.shape[0] > 0, \"test is empty\"\n\nscaler = MinMaxScaler()\nWIN_PERIOD = 1\nWIN_LENGTH = 5\nTIME_UNIT = \"min\"\n\ntensor = data_utils.transform_to_lstm_data(train, kpi, WIN_PERIOD, TIME_UNIT, WIN_LENGTH, scaler)\ntest_tensor = data_utils.transform_to_lstm_data(test, kpi, WIN_PERIOD, TIME_UNIT, WIN_LENGTH, scaler)\nprint(f\"(tensor shape: {tensor.shape})\")\nprint(f\"(test tensor shape: {test_tensor.shape})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import packages\nimport math\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import Sequence\nfrom datetime import timedelta\nfrom sklearn.metrics import mean_squared_error\n\nX = tensor[:, 1:, :]\ny = tensor[:, 0, :]\n\nX_test = test_tensor[:, 1:, :]\ny_test = test_tensor[:, 0, :]\n\n# Create the Keras model.\nn_steps = tensor.shape[1]-1\nn_features = tensor.shape[2]\n\n# vanilla lstm\n# ts_inputs = tf.keras.Input(shape=(n_steps, n_features))\n# x = layers.LSTM(units=100, activation='tanh')(ts_inputs)\n# x = layers.Dropout(0.2)(x)\n# outputs = layers.Dense(n_features, activation='linear')(x)\n# model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)\n\n# stacked lstm\n# ts_inputs = tf.keras.Input(shape=(n_steps, n_features))\n# x = layers.LSTM(units=100, return_sequences=True, activation='tanh')(ts_inputs)\n# x = layers.LSTM(units=100, activation='tanh')(x)\n# x = layers.Dropout(0.2)(x)\n# outputs = layers.Dense(tensor.shape[2], activation='linear')(x)\n# model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)\n\n# CNN into LSTM\n# ts_inputs = tf.keras.Input(shape=(n_steps, n_features))\n# x = layers.Conv1D(filters=64, kernel_size=4, activation='tanh')(ts_inputs)\n# x = layers.MaxPooling1D(pool_size=2)(x)\n# x = layers.LSTM(units=50, activation='tanh')(x)\n# x = layers.Flatten()(x)\n# x = layers.Dense(50, activation='relu')(x)\n# x = layers.Dropout(0.2)(x)\n# outputs = layers.Dense(tensor.shape[2], activation='linear')(x)\n# model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)\n# outputs = layers.Dense(n_features, activation='linear')(x)\n# model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)\n\n# CNN + LSTM\n# input1 = tf.keras.Input(shape=(n_steps, n_features))\n# input2 = tf.keras.Input(shape=(n_steps, n_features))\n# cnn = layers.Conv1D(filters=64, kernel_size=4, activation='tanh')(input1)\n# cnn = layers.MaxPooling1D(pool_size=2)(cnn)\n# cnn = layers.Flatten()(cnn)\n# lstm = layers.LSTM(units=100, activation='tanh')(input2)\n# lstm = layers.Flatten()(lstm)\n# x = layers.concatenate([cnn, lstm])\n# x = layers.Dropout(0.2)(x)\n# outputs = layers.Dense(n_features, activation='linear')(x)\n# model = tf.keras.Model(inputs=[input1, input2], outputs=outputs)\n\n# one lstm cell for each time series\ninputs = []\noutputs = []\nfor _ in range(n_features):\n    inputs.append(tf.keras.Input(shape=(n_steps, 1)))\n    x = layers.LSTM(units=5, activation='tanh')(inputs[-1])\n#     x = layers.Dropout(0.2)(x)\n    outputs.append(layers.Dense(1, activation='linear')(x))\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n# Specify the training configuration.\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['mse'])\n\n\n\n# FIT\nBATCH_SIZE = 128\n\nxx = [X[:,:,i] for i in range(n_features)]\nyy = [y[:, i] for i in range(n_features)]\nxx_test = [X_test[:,:,i] for i in range(n_features)]\nyy_test = [y_test[:, i] for i in range(n_features)]\nmodel.fit(x=xx, y=yy, epochs=20, batch_size=BATCH_SIZE, validation_data=(xx_test, yy_test))\n\nmodel.save('one_lstm_for_each_kpi_os_020.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train for all nodes and save models\nfrom sklearn.preprocessing import MinMaxScaler\n\nBATCH_SIZE = 128\nWIN_PERIOD = 1\nWIN_LENGTH = 5\nTIME_UNIT = \"min\"\n\ntraining_data_path = \"AIOps挑战赛数据\"\nsave_dir = \"./lstm2/\"\nfilename_format = \"lstm_{}.h5\"\nmodels = {}\nscalers = {}\ndates = [\"2020_05_22\", \"2020_05_23\", \"2020_05_24\", \"2020_05_25\",\"2020_05_26\",\"2020_05_27\",\"2020_05_28\", \"2020_05_29\",\"2020_05_30\",\"2020_05_31\"]\n\ndata_utils = DataUtils(verbose=True)\nhost, failures = data_utils.load_data(training_data_path, dates, None, 'host')\nnodes = host['cmdb_id'].unique()\n\nhost['cmdb_id'] = data_utils.node_le.fit_transform(host['cmdb_id'])\n\nfor node in nodes:\n    print(f\"===== {node} =====\")\n    train = host[host['cmdb_id'] == data_utils.node_le.transform([node])[0]]\n    kpi = train['name'].unique()\n\n    # prepare data\n    print(\"Preparing data\")\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    tensor = data_utils.transform_to_lstm_data(train, kpi, WIN_PERIOD, TIME_UNIT, WIN_LENGTH, scaler)\n    X = tensor[:, 1:, :]\n    y = tensor[:, 0, :]\n\n    # create the Keras model.\n    ts_inputs = tf.keras.Input(shape=(tensor.shape[1]-1, tensor.shape[2]))\n    x = layers.LSTM(units=50, activation='tanh')(ts_inputs)\n    x = layers.Dropout(0.2)(x)\n    outputs = layers.Dense(tensor.shape[2], activation='linear')(x)\n    model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)\n\n    # specify the training configuration.\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n                metrics=['msle'])\n\n    # fit model \n    print(\"Fitting\")\n    model.fit(x=X, y=y, batch_size=BATCH_SIZE, epochs=30, validation_split=0.8, workers=4, use_multiprocessing=True)\n\n    # save model and scalers\n    print(\"Saving\")\n    models[node] = model\n    scalers[node] = scaler\n    filename = save_dir + filename_format.format(node)\n    model.save(filename)\n\n# pickle.dump(models, open(\"all_lstm_models.pkl\", 'wb'))\npickle.dump(scalers, open(\"all_scalers.pkl\", 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r lstm2.zip ./lstm2/ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"THRESHOLD = 1\nkpi_name= np.sort(test['name'].unique())\n\ny_predicted = model.predict(X_test)\nmse = ((y_predicted-y_test)**2).mean(axis=0)\nmsle = ((np.log1p(y_predicted)-np.log1p(y_test))**2).mean(axis=0)\nanomalous_kpis = np.where(mse > THRESHOLD)\nkpi_name[anomalous_kpis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot kpis true vs predicted\nkpi_name= np.sort(test['name'].unique())\ny_predicted = model.predict([X_test[:,:,i] for i in range(n_features)])\n\nxinput = pd.DataFrame(data=X_test[:, 0, :], columns=kpi_name)\ntrue = pd.DataFrame(data=y_test, columns=kpi_name)\npredicted = pd.DataFrame(data=np.concatenate(y_predicted, axis=1), columns=kpi_name)\n\nn_figs = 5\n# plot_input = xinput.iloc[:, -n_figs:]\nplot_true = true.iloc[:, -n_figs:]\nplot_predicted = predicted.iloc[:, -n_figs:]\n\nfig, ax = plt.subplots(n_figs, 2, figsize=(20, 20))\n# plot_input.plot(ax=ax[:, 0], subplots=True, sharey=False, xlabel=\"input\")\nplot_true.plot(ax=ax[:, 0], subplots=True, sharey=False, sharex=True, xlabel=\"true\")\nplot_predicted.plot(ax=ax[:, 1], subplots=True, sharey=False, xlabel=\"predicted\")\nfig.suptitle(\"KPIs of host 'os_020'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig.savefig(\"one lstm for every kpi - batch100 units5 nodropout\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse = ((y_test-np.concatenate(y_predicted, axis=1))**2).mean(axis=0)\nkpi_name[np.where(mse > 0.2)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.models.load_model(\"lstm2/lstm_db_003\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trace Anomaly Detection\n"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data_utils = DataUtils(verbose=True)\ntrace_df, failures = data_utils.load_data(training_data_path, [\"2020_05_23\"], None, 'trace')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate anomaly score\ndef anomaly_score(traces):\n    print(f\"Anomaly scores of {len(traces['trace_id'].unique())} traces\")\n    \n    if 'node_name' not in traces.columns:\n        traces['node_name'] = traces['cmdb_id'] # CSF, FlyRemote, OSB, RemoteProcess\n        traces.loc[trace_df['call_type'].isin(['LOCAL', 'JDBC']), 'node_name'] = traces.loc[trace_df['call_type'].isin(['LOCAL', 'JDBC']), 'ds_name']\n\n    def f(x):\n        d = pd.Series({\n            'start_time': x['start_time'].min(),\n\n            # elapsed_time\n            'elapsed_time': x['elapsed_time'].mean(),\n            'childs_sum_elapsed_time': x['elapsed_time_child'].sum(),\n            'childs_average_elapsed_time': x['elapsed_time_child'].mean(),\n            'parent_average_elapsed_time': x['elapsed_time_parent'].mean(),\n\n            # call_time\n            'parent_call_time': x['elapsed_time_parent'].max() - x['elapsed_time_cousin'].sum(),\n            'incoming_call_time': (x['elapsed_time_parent'].max() - x['elapsed_time_cousin'].sum()) / x['id_cousin'].count(),\n            'outgoing_call_time': x['elapsed_time'].mean() - x['elapsed_time_child'].sum(),\n            'mean_outgoing_call_time': (x['elapsed_time'].mean() - x['elapsed_time_child'].sum()) / x['pid_child'].count() if x['pid_child'].count() > 0 else 0,\n\n            # number of ...\n            'number_of_childs': x['pid_child'].count(),\n            'number_of_cousins': x['id_cousin'].count(),\n\n            # success_rate\n            'child_success_rate': x['success_child'].mean(),\n            'parent_success_rate': x['success_parent'].mean(),\n            'cousin_success_rate': x['success_cousin'].mean(),\n            'success': x['success'].mean(),\n\n            # names\n            'childs_name': x['node_name_child'].dropna().unique().tolist(),\n            'parents_name': x['node_name_parent'].dropna().unique().tolist(),\n            'cousins_name': x['node_name_cousin'].dropna().unique().tolist(),\n        })\n        return d\n    \n#     # childs\n#     s = traces.merge(traces[['trace_id', 'pid', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'id'], right_on=['trace_id', 'pid'], suffixes=('', '_child'))\n    \n#     # parents\n#     s = s.merge(traces[['trace_id', 'id', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'pid'], right_on=['trace_id', 'id'], suffixes=('', '_parent'))\n    \n#     # cousins\n#     s = s.merge(traces[['trace_id', 'pid', 'id', 'elapsed_time', 'node_name', 'success']], how='outer', left_on=['trace_id', 'pid'], right_on=['trace_id', 'pid'], suffixes=('', '_cousin'))\n    childs = traces.merge(traces[['trace_id', 'pid', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'id'], right_on=['trace_id', 'pid'], suffixes=('', '_child'))\n    parents = traces.merge(traces[['trace_id', 'id', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'pid'], right_on=['trace_id', 'id'], suffixes=('', '_parent'))\n    cousins = traces.merge(traces[['trace_id', 'pid', 'id', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'pid'], right_on=['trace_id', 'pid'], suffixes=('', '_cousin'))\n    \n    s = pd.concat([childs, parents, cousins])\n    print(f\"Merging complete parents, childs and cousin are now reunited ! Applying custom scoring function on dataframe of shape {s.shape}\")\n    anomaly_score = s.groupby(['node_name', 'trace_id', 'id']).apply(f)\n    anomaly_score.fillna(0, inplace=True) # nan are nodes without childs, or without parents. Thus all ..._time related to either of them is 0 (and not NaN)\n    return anomaly_score\n# main\nanomaly_scores = anomaly_score(trace_df)\nanomaly_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing cell\nchilds = trace_df.merge(trace_df[['trace_id', 'pid', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'id'], right_on=['trace_id', 'pid'], suffixes=('', '_child'))\nparents = trace_df.merge(trace_df[['trace_id', 'id', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'pid'], right_on=['trace_id', 'id'], suffixes=('', '_parent'))\ncousins = trace_df.merge(trace_df[['trace_id', 'pid', 'id', 'elapsed_time', 'node_name', 'success']], how='left', left_on=['trace_id', 'pid'], right_on=['trace_id', 'pid'], suffixes=('', '_cousin'))\n\nprint(f\"shapes {childs.shape} {parents.shape} {cousins.shape}\")\npd.concat([childs, parents, cousins])\n\n####\nquantile = trace_df['elapsed_time'].quantile(0.99)\ntrace_df['outside_quantile'] = trace_df['elapsed_time'] > quantile\ntrace_roots = trace_df[trace_df['pid'] == 'None']\ntrace_roots.groupby(\"trace_id\").sum().sort_values(by='elapsed_time', ascending=False)\ntrace_df[trace_df['trace_id'] == '38171490e1640892ee7b'].sort_values(by='start_time')\ntrace_df.groupby(\"trace_id\").sum().sort_values(by='elapsed_time', ascending=False)\n\n####\nplot_trace(trace_df[trace_df['trace_id'] == '6a171001cb3255425da7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT TRACE\nfrom graphviz import Digraph\ndef plot_trace(trace):\n    u = Digraph('unix', format='png', strict=True)\n    names = trace[['id', 'cmdb_id', 'service_name', 'ds_name']].set_index('id')\n    for index, row in trace.iterrows():\n        child_id = row['id']\n        parent_id = row['pid']\n\n        parent = names.loc[parent_id][['cmdb_id', 'service_name', 'ds_name']] if parent_id in names.index else {'cmdb_id': parent_id, 'service_name': parent_id}\n        child = names.loc[child_id][['cmdb_id', 'service_name', 'ds_name']]\n        \n        if row['call_type'] in ['JDBC']:\n            u.edge(str(parent['cmdb_id']), str(child['ds_name']), label=row['call_type'])\n#         elif row['call_type'] in ['CSF']:\n#             u.edge(str(parent['cmdb_id']), str(child['service_name']), label=row['call_type'])\n        elif row['call_type'] in ['FlyRemote', 'OSB', 'CSF']:\n            # skip these call types\n            continue\n        else:\n            u.edge(str(parent['cmdb_id']), str(child['cmdb_id']), label=row['call_type'])\n        \n    return u\ntraces = trace_df[trace_df['trace_id'] == '6a1713243e3266b9c7f7']\n# anomaly_time = datetime.datetime.strptime(\"23/05/2020 16:05:00\", \"%d/%m/%Y %H:%M:%S\")\n# window = datetime.timedelta(minutes=20)\n# traces = trace_df[(trace_df['start_time'] - anomaly_time).abs() < window]\ndrawing = plot_trace(traces)\ndrawing.render('drawing')\ndrawing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate mean value of last n minutes (20min)\npast_mean_scores = anomaly_scores.mean(axis=0, level=0)\npast_mean_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot anomaly_scores against time\ndata = s[['elapsed_time', 'childs_average_elapsed_time', 'call_time','start_time', 'cmdb_id_parent', 'child_success_rate']].melt(\n    ['start_time', 'cmdb_id_parent'], var_name='cols', value_name='value')\ng = sns.FacetGrid(data=data, row='cmdb_id_parent',  hue='cols', sharex=True, sharey=False, height=3, aspect=4)\ng.map_dataframe(sns.lineplot, x='start_time', y='value')\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FindRootCause function\ndef find_root_cause(anomaly_scores, past_mean_scores):\n    scores = anomaly_scores.dropna().reset_index(['trace_id', 'id'], drop=True)\n    anomalies = []\n    for node in scores.index.unique():\n        node_anomaly_scores = (scores.loc[node, past_mean_scores.columns] - past_mean_scores.loc[node, :]) / past_mean_scores.loc[node, :]\n        print(node_anomaly_scores)\n        \n        # Network loss (docker/os)\n        network_loss_threshold = 0.8\n        # if sent_queue or receive_queue is anomalous:\n        if node_anomaly_scores['outgoing_call_time'] > network_loss_threshold and node_anomaly_scores['incoming_call_time'] > network_loss_threshold:\n            anomalies.append([node, \"Sent_queue\"])\n            anomalies.append([node, \"Received_queue\"])\n        \n        # CPU fault (docker)\n        cpu_fault_threshold = 0.8\n        if node_anomaly_scores['outgoing_call_time'] > network_loss_threshold and node_anomaly_scores['incoming_call_time'] > network_loss_threshold:\n            anomalies.append()\n        \n        # Network delay (docker/os)\n        network_delay_threshold = 0.8\n        \n        # DB connection limit (db)\n        db_connection_limit_threshold = 0.8\n        \n        # DB close (db)\n        db_close_threshold = 0.8\n        \n    return anomalies\nscores_to_test = anomaly_scores.sort_values(by='start_time').head(50000)\nfind_root_cause(scores_to_test, past_mean_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_to_test = anomaly_scores[(anomaly_scores['start_time']-datetime.datetime.strptime(\"23/05/2020 16:05:00\", \"%d/%m/%Y %H:%M:%S\")).abs() < datetime.timedelta(minutes=1)]\nk = scores_to_test.reset_index(['trace_id', 'id'], drop=True)\nk = (k.loc[:, past_mean_scores.columns] - past_mean_scores) / past_mean_scores\nk = k.dropna().max(level=0)\nk.sort_values(by=['incoming_call_time', 'outgoing_call_time'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalous_on_off_state_idx = np.where(host.loc[host['name'] == 'On_Off_State', 'value'] < 1)\nanomalous_db = host.iloc[anomalous_on_off_state_idx]['cmdb_id'].to_list()\nanomalous_db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"host.loc[host['name'] == 'Sent_queue', ['value', 'cmdb_id', 'timestamp']].groupby('cmdb_id').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### zscore test\ntraces = trace_df.copy()\ntraces['node_name'] = traces['cmdb_id'] # CSF, FlyRemote, OSB, RemoteProcess\ncondition = traces['ds_name'].fillna('').str.contains('db')\ntraces.loc[condition, 'node_name'] = traces.loc[condition, 'ds_name']\n\nanomaly_time = datetime.datetime.strptime(\"23/05/2020 16:40:00\", \"%d/%m/%Y %H:%M:%S\")\nts = traces[(traces['start_time']-anomaly_time).abs() < datetime.timedelta(minutes=1)]\nts = ts.loc[ts['node_name'] == \"db_003\", ['elapsed_time', 'start_time']].set_index('start_time')\nlong_time_ts = traces.loc[traces['node_name'] == \"db_003\", ['elapsed_time', 'start_time']].set_index('start_time')\nzscore = (ts - long_time_ts.mean()) / long_time_ts.std()\nzscore.sort_values('elapsed_time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = host[(host['timestamp']-datetime.datetime.strptime(\"23/05/2020 16:05:00\", \"%d/%m/%Y %H:%M:%S\")).abs() < datetime.timedelta(minutes=3)]\nsns.FacetGrid(h.loc[h['name'] == 'container_cpu_used', ['value', 'cmdb_id', 'timestamp']], col='cmdb_id').map_dataframe(sns.lineplot, x='timestamp',y='value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ALL TIME statistics\ndata_utils = DataUtils(verbose=True)\ndates = [\"2020_05_22\", \"2020_05_23\", \"2020_05_24\", \"2020_05_25\",\"2020_05_26\",\"2020_05_27\",\"2020_05_28\", \"2020_05_29\",\"2020_05_30\",\"2020_05_31\"]\ntrace_df, failures = data_utils.load_data(training_data_path, dates[:len(dates)//3], None, 'trace')\nprint(\"done!\")\n\nprint(\"node name\")\ntrace_df['node_name'] = trace_df['cmdb_id'] # CSF, FlyRemote, OSB, RemoteProcess\ncondition = trace_df['ds_name'].fillna('').str.contains('db')\ntrace_df.loc[condition, 'node_name'] = trace_df.loc[condition, 'ds_name']\nprint(\"done!\")\n\nprint(\"Free memory\")\ntrace_df = trace_df[['node_name', 'elapsed_time']]\ngc.collect()\nprint(\"done!\")\n\ntrace_df2, _ = data_utils.load_data(training_data_path, dates[len(dates)//3:(len(dates)//3)*2], None, 'trace')\nprint(\"done!\")\n\nprint(\"node name\")\ntrace_df2['node_name'] = trace_df2['cmdb_id'] # CSF, FlyRemote, OSB, RemoteProcess\ncondition = trace_df2['ds_name'].fillna('').str.contains('db')\ntrace_df2.loc[condition, 'node_name'] = trace_df2.loc[condition, 'ds_name']\nprint(\"done!\")\n\nprint(\"Free memory\")\ntrace_df2 = trace_df2[['node_name', 'elapsed_time']]\ngc.collect()\nprint(\"done!\")\n\ntrace_df = trace_df.append(trace_df2)\n\ntrace_df3, _ = data_utils.load_data(training_data_path, dates[(len(dates)//3)*2:], None, 'trace')\nprint(\"done!\")\n\nprint(\"node name\")\ntrace_df3['node_name'] = trace_df3['cmdb_id'] # CSF, FlyRemote, OSB, RemoteProcess\ncondition = trace_df3['ds_name'].fillna('').str.contains('db')\ntrace_df3.loc[condition, 'node_name'] = trace_df3.loc[condition, 'ds_name']\nprint(\"done!\")\n\nprint(\"Free memory\")\ntrace_df3 = trace_df3[['node_name', 'elapsed_time']]\ngc.collect()\nprint(\"done!\")\n\ntrace_df = trace_df.append(trace_df3)\n\nzscores = []\nstatistics = []\nfor node_name in trace_df['node_name'].unique():\n    print(f\"node {node_name}\")\n    ts = trace_df.loc[trace_df['node_name'] == node_name, ['elapsed_time']]\n    long_ts = trace_df.loc[trace_df['node_name'] == node_name, ['elapsed_time']]\n    mean, std = long_ts.mean(), long_ts.std()\n    quantile95, quantile99, quantile99_99 = long_ts.quantile(0.95), long_ts.quantile(0.99), long_ts.quantile(0.9999)\n    print(f\"ALL TIME - mean {mean.values}, std {std.values}, quantiles 95 {quantile95.values}, 99 {quantile99.values}, 99.99 {quantile99_99.values}\")\n    statistics.append({\n        'node_name': node_name,\n        'mean' : mean.values,\n        'std': std.values,\n        'quantile95': quantile95.values, \n        'quantile99': quantile99.values, \n        'quantile99_99': quantile99_99.values\n    })\n    \n#     temp_df = pd.DataFrame({\n#         'zscore': (ts - mean) / std\n#     })\n#     temp_df['node_name'] = node_name\n#     zscores.append(temp_df)\n# alltime_zscores = pd.concat(zscores)\nalltime_statistics = pd.DataFrame(statistics)\nalltime_statistics.to_csv(\"alltime_statistics.csv\", index=False)\nalltime_zscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"host.loc[host['name'] == 'Received_queue', ['value', 'timestamp']].describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}