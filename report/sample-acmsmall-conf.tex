%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall, screen, nonacm]{acmart}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[acmsmall,screen,review]{acmart}
%% Or use the sample-acmsmall-submission.tex file.
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{ANM project: DANM! team}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Aurelien Vu Ngoc}
% \authornote{Both authors contributed equally to this research.}
\email{vunaa10@mails.tsinghua.edu.cn}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}
  
\author{Fabien Ravet}
% \authornote{Both authors contributed equally to this research.}
\email{@mails.tsinghua.edu.cn}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}
% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Microservices-operated web services are extremely difficult to monitor and troubleshoot manually, that is why AIOps research are gaining importance recently. This work studies different methods to anomaly detection and root cause analysis in a web service environment. With the help of various performance indicators, we created two models capable of efficiently detecting outliers: the first one uses time series forecasting through a LSTM neural network with a moving average anomaly detection. The second model is hard coded with a set of rules to discover anomalous patterns in the live data. This report demonstrates the main steps of our work on the 2020 ANM project with explicit insights on the utilized methods, clear visual figures and in-depth discussion of the major issues encountered during the process. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003033.10003068</concept_id>
       <concept_desc>Networks~Network algorithms</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Networks~Network algorithms}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Advanced Network Management, Anomaly Detection, Time Series Outlier Detection}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
This project aims to design a performant AIOps algorithm to take care of the troubleshooting of a given web service. 
This work includes taking into consideration the many complex call relations between each microservice and make good use of the various available performance indicators, all while being a live algorithm capable of reacting in real-time. 
This implies having the capacity to trace back in time the origin of potential anomalies before making precise and quick decisions to help the system recover, and also further develop abilities to recognize potential threats to prevent the system from failing at all. 
More specifically, this project focuses on pinpointing the anomalies when they happen and analyzes the provided data to find the root cause of the event. 

\section{Problem Background and Related Work}
Digitalization has come to a point where systems are fully interconnected with each other, thus relying on one another to perform as well as possible. 
Many systems are based on a microservice approach where each subcomponent has a very specific role inside the global work chain. 
Similar to an assembly line work in the 1900s, this method provides high performance because microservices are specialists at their own task while allowing a heavy global workload because all tasks can be achieved simultaneously.
\\
As appealing as this method can appear at first glance, it comes with some incoveniences and problems as well: the major concern is cascading microservice failures resulting in a global failure. 
In order to prevent the system from failing, companies were originally hiring system engineers to monitor and repair the system, but with the exponential growth of web services and their various microservices, it has become impossible to monitor all of them at once. 
That's where AIOps comes in to save the day, using artificial intelligence recent progress to make this monitoring task less tedious, more precise and eventually faster.
\\
Many research have been conducted to design optimal solutions to this kind of problem. Reviews of the best methods are numerous as well:  various approaches have been considered throughout history, including autoencoders, to tackle the absence of clean training data issue \cite{zhou_anomaly_2017} and other unsupervised learning techniques through LSTM networks like in \cite{nedelkoski_anomaly_2019}.
Clustering-based algorithms and other nearest neighbors algorithms are also considered to be very robust for anomaly detection \cite{chandola_anomaly_2009}, and have even been furthered explored to develop specialized methods such as BIRCH in \cite{gulenko_detecting_2018}. 
Troubleshooting an aggregate of microservices working together is a non-trivial task that can be performed through an comprehensive analysis of the trace call graph, like in \cite{manzoor_fast_2016}, that need to be extended with root cause investigation such as \cite{weng_root_2018} or \cite{yan_g-rca_2012}. 

\section{General Architecture}
First let's examine the architecture of the web service. 
Composed of multiple microservices with well-defined APIs that communicate, exchange and interact with each other, the web service produces 3 different kinds of data to deal with:
\begin{itemize}
  \item ESB business indicator data
  \item Trace data
  \item Hosts Key Performance Indicators (KPIs) data 
\end{itemize}
Through a Tencent Cloud Virtual Machine, our program is able to consume real-time data retrieved from a Kafka broker, to analyze these different sources of information as well as to detect anomalies and their root cause within 5 minutes after the anomaly occurrence. 
\\
Below we report our solution to this project for the different rounds of evaluation. 
During the first round, we tried to make use of the ESB data in order to discover anomalies, before analyzing the hosts KPIs to retrieve the source of the anomaly. 
Whereas in the second and third rounds, our approach was different and, learning from the other groups' presentations, we decided to switch to a method based on the analysis of the trace data to be more reactive. 
   


\section{Round 1 Solution}
This section explicits the details of the first round solution we implemented on the server. 
Unfortunately the final result is zero due to compatibility issues we encountered with the environment on the Tencent Cloud VM that we did not realize was causing our program to fail. 
An organizational problem within the team was a major factor in this miscalculation. 

\subsection{Anomaly Detection using Business Index}
The first step of this solution is to discover anomalies using ESB data, which can be achieved through multiple methods including an exponential moving average (EMA), and an unsupervised method specially developped for time series anomaly detection, HBOS (histogram based outlier detection \cite{goldstein_histogram-based_2012}). 
Both methods are extremely easy to implement, the former being the easiest because it does not require any model to be trained, only statistical consideration are required.
We decided this solution should preferably use the \verb|avg_time| feature of the ESB data, rather than the \verb|success_rate|, because unsuccessful business index are extremely rare and it does not help finding anomalies that much. 
Even though the HBOS approach showed promising results in the first place (see \autoref{fig:hbos}), we finally opted for the EMA method. 
EMA can be implemented effortlessly, showed excellent results (see \autoref{sec:appendix_ema}) while being free from any dependencies (as opposed to HBOS which is not implemented in the standard python libraries).

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/hbos.png}
  \caption{HBOS (histogram based outlier detection) showed promising initial results, being able to successfully discover major ESB data anomalies.}
  \label{fig:hbos}
\end{figure}

\subsection{Candidate Anomalous Host}
Once our program correctly detected an anomaly, we needed to draw out candidate anomalous hosts for further testing and analysis. 
We used Trace data to complete this step by analyzing the call time of each node in the call chain graph using the \verb|elapsed_time| of each node. 

\subsection{Anomalous Key Performance Index Retrieval}
Originally, this step of retrieving the anomalous KPIs from the anomalous node was also trying to find the root cause. 
Finding anomalous behaviours in KPIs would lead to a ranking of the most "anomalous" behaviours that would later be flagged as the source host of the anomaly. 
However the results did not follow and we found that having the root cause analysis step provided better results. 
\\
\\
The core idea behind this first round's solution is to use machine learning models to predict KPIs values and compared them to the received KPIs values from the server. 
The difference between the true and the predicted value that would force the KPI into being considered anomalous, is measured using a predefined threshold on an error function. 
We tried various error functions, including: mean absolute error (MAE), mean relative error (MRE), mean squared error (MSE), binary cross-entropy.
\\
\\
The proper functioning of this step essentially resides in the choice of the model that will make such prediction. 
Since the problem deals with Time Series, there are many models indicated for this type of use including statistical methods (such as ARIMA and its multiple variants \cite{moayedi_arima_2008}),  decision trees (such as XGBoost and LigthGBM) or deep learning (such as LSTM based neural networks and other deep learning techniques \cite{kim_web_2018}). 
Additionally, the methods detailed below require some data transformation and feature engineering. 
Incoming data from the server is modified into a "supervised dataset" where the input has columns for each KPI time series, further divided into past values (at times t-1min, t-2min, ...) and output has one columns for each KPI time series with only the current value (t). 
In order to tackle the missing values issue, we decided to go for forward-fill imputing because every KPI has different update frequency and we considered missing values as an absence of update, i.e. the value has not changed up until now.
\\
\subsubsection*{LSTM}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{images/lstm.png}
  \caption{Architecture of a LSTM model to forecast KPIs values of a given host (true/predicted)}
  \label{fig:lstm_architecture}
\end{figure}

Long Short Term Memory (LSTM) cells are optimal for time series prediction, in fact, a unique cell is capable of forecasting a time series with very high accuracy. 
Therefore using such a model on KPIs seems appropriate and we decided to assign one model to each host with each model composed of n independent cells in parallel, n being the number of KPIs describing the given host (see \autoref{fig:lstm_architecture}).
However, because LSTM need supervised datasets as input, we have to transform the time series dataset beforehand, which can be achieved through a succession of pandas operations on the data. 
\\
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{images/lstm_model.png}
  \caption{KPIs: Time series forecasting using LSTM cells}
  \label{fig:lstm}
\end{figure}
In addition to this design, we also tried various LSTM architecture including Vanilla LSTM, stacked LSTM, CNN together with LSTM, but the single cell design was sufficient.
LSTM time series forecasting showed promising results that can be improved with further tuning and analysis (see \autoref{fig:lstm}). 
But this method was designed to work hand in hand with the candidate anomalous host step, that was lacking efficiency when it was implemented. 
In the sake of time and of simplicity, we decided to abandon this approach in the next rounds. 

\subsubsection*{XGBoost}
XGBoost is an extremely popular algorithm to solve structured supervised problems that have recently proven to be particularly effective, and rather simple to implement.
Again, the given problem does not label exactly as "supervised problem", so we had to transform the data to fit xgboost input.
Adapting xgboost to the given problem was also challenging because of the strategy to adopt: how many models do we need? 
Since xgboost cannot handle categorical data well \cite{chen_xgboost_2016} (e.g. host name, KPI name\dots), do we need to have a model for each? 
We came up with 2 solutions: use a multioutput wrapper for xgboost to support all hosts and KPIs at once, or switch to LightGBM.
Though we tried the multiple xgboost models approach as well as the multiouput single xgboost model approach, the results were not as promising as LSTM. 
\autoref{fig:xgboost} in \autoref{sec:appendix_xgboost} shows a sample of predicted kpis using the multioutput xgboost model where some predictions are not that accurate.

\section{Round 2 Solution}
We worked on our own to design rule-based algorithms mainly analysing trace data to find the anomalous nodes and the KPI affected. Due to a lack of communication from Fabien, we came up with two slightly different approaches.

\subsection{Aurelien's implementation}
This time around anomalies are detected using trace and host KPIs data only because the business index data has a low frequency update that can make the program skip anomalies. 
Inspired by the best solutions method, we switched back to a rule-based model.
A set of rules analyzes the trace data using different engineered features including: the \verb|elapsed_time| (that is natively inside the trace data), the \verb|call_time| of hosts (i.e. the time spent in edges of the trace call graph), the position of hosts in the call graph (child, parent, cousin) and their own features. 
% insert table with the engineered features
\\
Our program first compares the \verb|elapsed_time| of nodes with all-time recorded values and flags the host as anomalous if its value goes over a certain threshold (the 99.99\% quantile).
Then a second layer of tests using a z-score analysis inside a 20 minutes moving window ($zscore = (x-mean) / std$) can detect anomalies by comparing the computed z-score to a specific threshold determined using the training data.
Since the z-score gives an estimation of how far a value inside a series is from its mean (more specifically, how many standard devation away this value is from the mean), this indicator is easily interpretable and delivers good results. 
\\
Afterwards, there is a series of specific tests for particular configuration and well-defined anomalies such as "db close" error, "db connection limit" or "os network delay".
These errors have associated KPIs that are easily tractable: \verb|Sent_queue| and \verb|Received_queue| for the os anomalies ; \verb|On_Off_State| and \verb|tnsping_result_time| for the db faults. 
Sudden peaks have tremendous influence on the system and should be considered outliers. 
We monitored such peak events through a simple upper bound threshold determined using the training labeled data.  
\\
Then comes the engineered features. 
The core idea behind this approach is to find patterns in the fixed call chain graph that induces faults and errors. 
\autoref{fig:call_graph} shows how microservices call each other. 
This allows manual analysis using the labeled data to discover anomalous patterns, including:
\begin{itemize}
  \item high \verb|call_time| often implies a network issue (delay or loss), depending on the status of KPIs \verb|Sent_queue| and \verb|Received_queue|
  \item increasing incoming and outgoing \verb|call_time| often translates as an intern problem (such as CPU usage issue)
  \item \dots
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/call_graph.png}
  \caption{The web service architecture has a fixed call graph.}
  \label{fig:call_graph}
\end{figure}

\subsection{Fabien's implementation}
This solution is aimed to be working on the training and testing data provided, and has not yet been implemented on the Tencent VM.
\\
\subsubsection*{Microservice Architecture}
As microservice-based softwares grow in popularity, such architecture generates new problems, and thus new approaches are needed to manage them. A microservice is a small scalable software service which is part of a larger application. The intermittent communication between the services makes it difficult to identify the root cause of a problem, so it is important to first study the specific architecture of the application and visualize the connections between microservices, before addressing log anomaly detection and location.
\\
After running some tests on the trace data, we can understand the relations between the different types of calls (see \autoref{fig:Call_chain_type} and \autoref{fig:Call_chain_host}). For example, a span with a Remote Process call type can only have span children with Fly Remote, CSF or Local call types. Furthermore, without much surprise, only the Remote Process spans have a parent span on a different host.
More importantly, we can analyze the calls between nodes to assert their relations, which will be important to address network anomalies for example.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.3\textwidth]{images/Call_chain_type.png}
  \caption{The hierarchical relations between call types.}
  \label{fig:Call_chain_type}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/Call_chain_host.png}
  \caption{The hierarchical relations between nodes.}
  \label{fig:Call_chain_host}
\end{figure}

\subsubsection*{Anomaly detection with ESB}
We kept using ESB business rate indicators to tackle the anomaly detection problem. Although the number of submitted requests and the success rate are not reliable enough to find all anomalies, the average time spent processing a request always matches the upcoming of an anomaly. Moreover, it doesn't induce much latency compared to the evolution of trace global processing time.




Exponential moving average is a great tool that detects the appearance of an anomaly down to the minute (see \autoref{fig:EMA}). It takes some time to stabilize after an anomaly, thus resulting in an increase of the expected duration of the anomaly. However, this is not a problem since the anomalies duration is 5 minutes long and the continuation of the algorithm only requires 2 minutes of anomalous traces.

\newpage
\subsubsection*{Data preprocessing}
A host can be considered anomalous if the running time of its spans is very low or high. In order to obtain the true running time of a microservice on its host, we first need to retrieve the children spans elapsed time from its own elapsed time. For that purpose, we group trace data according to trace id and sort spans according to their start time to easily find their parent.
\\
\subsubsection*{Defining abnormal call times}
We can build a matrix matching the hosts (columns) with the instances they call (rows), and fill it with the percentile of the running times of the corresponding spans. Setting a threshold for each type of call, we are able not only to detect the anomalous hosts, but also the very nature of the calling problem. This matrix is conceived on non anomalous trace data in order to set coherent thresholds, we built several of them with different percentiles and compared them later with the anomalous traces.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/Percentile_matrix.png}
  \caption{The 99.9th percentile matrix.}
  \label{fig:Percentile_matrix}
\end{figure}

\subsubsection*{Anomaly location}
For each anomaly detected, we select two minutes of anomalous trace data already preprocessed. We initialize another matrix matching the hosts and call types and use it to count the anomalous spans. A span is considered anomalous as soon as its calling time exceeds the percentile thresholds defined on the non anomalous training trace data.
\\
As there are very few different types of anomalous KPIs we can try to deduce from the matrix of abnormal spans both the host and type of affected KPI according to different distribution scenarios.
\\
The easiest type of failing KPI to detect is \verb|container_cpu_used| which triggers anomalous local calls in docker hosts.
On the other hand, the network problems affecting docker hosts is detected if a docker instance has huge remote process running time.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/Anomaly_location.png}
  \caption{The docker 3 calls on itself are anomalous: container CPU used error.}
  \label{fig:Anomaly_location}
\end{figure}

\section{Work allocation}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & Aurelien VU NGOC & Fabien RAVET \\ \hline
round 1 & HBOS, xgboost, LSTM & EMA for esb anomaly detection \\ \hline
round 2 \& 3 & \begin{tabular}[c]{@{}c@{}}Rule-based model\\ (implemented in Tencent VM)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Another rule-base model \\ (not yet implemented in Tencent VM)\end{tabular} \\ \hline
other & \begin{tabular}[c]{@{}c@{}}This report\\ Presentation\\ Tencent VM code design\end{tabular} & Presentation \\ \hline
\end{tabular}
\caption{Project work allocation}
\label{tab:work_allocation}
\end{table}

An overview of the work allocation can also be found in the GitHub commit history at the following url: \url{https://github.com/DANM-team/ANM-project}.

\section{Conclusion}
This study aims to implement a troubleshooting algorithm for a microservices-operated web service. 
We developped the many solutions we tried during the semester, discussed the different approaches and their pro \& cons, as well as analyzed the issues we faced. 
Our final solution is a rule-based approach to outlier detection that uses a set of directives, preliminarily determined using the provided labeled training data, to discover anomalies. 
It is helped by engineered features as well as specific hard coded patterns to perform accurate and fast anomaly detection. 
\\
All in all, though we wished for better results in the final test, conducting this project has been an unprecedented opportunity to work with real data coming straight from influencial real-life service. 
The many research and testing that have staked out our journey have been a priceless chance to understand the challenges and major concerns in AIOps. 
Finally we believe the ANM project has been a real occasion to put all the knowledge accumulated throughout the semester into practice, and more importantly, to let this practical and truly valuable experience serve future problems.   


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{final_report_references}

%%
%% If your work has an appendix, this is the place to put it.
\newpage
\appendix
\section{EMA results}\label{sec:appendix_ema}
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/EMA.png}
  \caption{The exponential moving average present dissimilarities on the anomaly.}
  \label{fig:EMA}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics{images/Anomaly_timestamps.png}
  \caption{EMA promising results: the timestamps of occurring anomalies exactly correspond to given anomalies in the training data.}
  \label{fig:anomaly_timestamps}
\end{figure}


\section{XGBoost results}\label{sec:appendix_xgboost}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{images/multioutput_xgboost_os020.png}
  \caption{Sample of Multioutput xgboost forecasting KPIs of $os020$ (true/predicted)}
  \label{fig:xgboost}
\end{figure}


\end{document}
\endinput
%%
%% End of file `sample-acmsmall-conf.tex'.
